# FinSync Pro: Complete Interview Preparation Guide
## A Deep Dive into Building Production-Ready RAG Systems for Financial AI

> **Purpose:** This tutorial is designed to help you understand every component, every decision, and every line of reasoning in this project. By the end, you'll be able to confidently explain the entire system architecture and answer any technical interview question about RAG systems.

---

# Part 1: Understanding RAG - The Foundation

## What is RAG and Why Do We Need It?

**RAG = Retrieval-Augmented Generation**

Think of it this way:
- **Pure LLM (GPT-4)**: Very smart, but only knows what it was trained on. Can't access your company's internal documents. Can't give you today's stock price.
- **RAG System**: An LLM that can "look things up" before answering. It retrieves relevant information first, then generates an answer based on that.

### The Problem RAG Solves

```
âŒ Without RAG:
User: "What's my FD interest rate?"
LLM: "I don't have access to your personal documents."

âœ… With RAG:
User: "What's my FD interest rate?"
System: [Searches user's uploaded policy PDF]
        [Finds: "Fixed Deposit Rate: 7.5% per annum"]
LLM: "Your FD interest rate is 7.5% per annum."
```

### Why RAG for Financial Advice?

1. **Accuracy**: Financial advice MUST be accurate. RAG grounds answers in actual documents.
2. **Personalization**: Each user has their own policies, statements, and documents.
3. **Real-time Data**: Stock prices, interest rates, and news change constantly.
4. **Auditability**: When the AI says "your rate is 7.5%", we can show exactly WHERE it found that.

---

## The High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER INTERFACE                          â”‚
â”‚                  (React + Tailwind + Framer Motion)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼ HTTP API Calls
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         FASTAPI BACKEND                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ /api/upload  â”‚  â”‚ /api/chat    â”‚  â”‚ /api/documents       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â–¼                      â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DOCUMENT PIPELINEâ”‚  â”‚   RAG AGENT      â”‚  â”‚  WEB SEARCH      â”‚
â”‚ PDF â†’ Chunks â†’   â”‚  â”‚ (GPT-4o-mini)    â”‚  â”‚  (Tavily API)    â”‚
â”‚ Embeddings       â”‚  â”‚ Tool Selection   â”‚  â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                      â”‚
           â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   VECTOR STORE   â”‚  â”‚ HYBRID RETRIEVER â”‚
â”‚   (FAISS)        â”‚  â”‚ FAISS + BM25     â”‚
â”‚   Semantic Searchâ”‚  â”‚ + RRF Fusion     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Key Interview Question: "Walk me through the request flow"

**Answer:**

1. User types "What's my FD rate?" in the React frontend
2. Frontend sends POST request to `/api/chat` with the query
3. FastAPI endpoint receives the request
4. The RAG Agent analyzes the query
5. Agent decides to use `document_search` tool (because "my" implies personal docs)
6. Hybrid Retriever searches FAISS (semantic) + BM25 (keyword)
7. Results are combined using Reciprocal Rank Fusion
8. Top chunks are passed to GPT-4o-mini
9. GPT generates a concise answer based on the retrieved context
10. Response is returned to frontend with answer, sources, and citations

---

# Part 2: The Document Ingestion Pipeline

## Why This Matters for Interviews

This is often the FIRST thing interviewers ask: "How do you get documents into the system?"

## Step-by-Step Pipeline

### Step 1: PDF Upload (User Action)

```
User drags "policy.pdf" onto the upload zone
         â”‚
         â–¼
Frontend creates FormData with the file
         â”‚
         â–¼
POST /api/upload with multipart/form-data
```

### Step 2: FastAPI Receives the File

```python
# From main.py
@app.post("/api/upload")
async def upload_document(file: UploadFile = File(...)):
    # 1. Validate it's a PDF
    if not file.filename.endswith('.pdf'):
        raise HTTPException(400, "Only PDF files are supported")
    
    # 2. Save to disk temporarily
    file_path = UPLOAD_DIR / file.filename
    with open(file_path, "wb") as f:
        content = await file.read()
        f.write(content)
    
    # 3. Process the document (this is where the magic happens)
    doc_id, documents = process_document(str(file_path), file.filename)
```

**Interview Question:** "Why save to disk first? Why not process in memory?"
**Answer:** PDFs can be large. Saving to disk prevents memory issues and allows for retry if processing fails. Also, `pypdf` library reads from file paths more efficiently.

### Step 3: PDF Text Extraction

```python
# From ingestion/document_processor.py
from pypdf import PdfReader

def extract_text_from_pdf(file_path: str) -> List[Tuple[str, int]]:
    """Extract text with page numbers for citations."""
    reader = PdfReader(file_path)
    pages = []
    
    for i, page in enumerate(reader.pages):
        text = page.extract_text()
        if text and text.strip():
            # Important: Keep page number for citations!
            pages.append((text, i + 1))
    
    return pages
```

**Why Keep Page Numbers?**
- When the AI says "7.5% interest rate", we need to show "From page 1"
- Citations build user trust
- Required for compliance in financial applications

### Step 4: Text Chunking (Critical Concept!)

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

def get_text_splitter():
    return RecursiveCharacterTextSplitter(
        chunk_size=500,         # Each chunk is ~500 characters
        chunk_overlap=50,       # 50 chars overlap between chunks
        length_function=len,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
```

**Interview Deep Dive: Why Chunk?**

```
âŒ Problem with full documents:
- Embedding models have token limits (512 tokens typically)
- Retrieving a 100-page PDF when user asks about ONE thing is wasteful
- LLM context windows get overwhelmed

âœ… Solution: Chunking
- Break document into small, focused pieces
- Each chunk is independently searchable
- Retrieve only the 4-5 most relevant chunks
```

**Why 500 characters?**
```
Too small (100): "Fixed Deposit" in one chunk, "Rate: 7.5%" in another
                 Search might miss the connection!

Too large (5000): Defeats the purpose, too much irrelevant info

500 chars: Sweet spot - captures a complete thought/section
```

**Why Overlap?**
```
Without overlap:
Chunk 1: "...the Fixed Deposit Rate"
Chunk 2: "is 7.5% per annum..."
         â†‘ Important info split!

With 50-char overlap:
Chunk 1: "...the Fixed Deposit Rate is 7.5%..."
Chunk 2: "Rate is 7.5% per annum. The minimum..."
         â†‘ Info preserved in both!
```

**Why RecursiveCharacterTextSplitter?**

It tries separators in order:
1. `\n\n` (paragraph breaks) - Best split points
2. `\n` (line breaks) - Second best
3. `. ` (sentences) - Third best
4. ` ` (words) - If needed
5. `` (characters) - Last resort

This ensures we split at natural boundaries, not mid-word.

### Step 5: Generate Embeddings

```python
from langchain_huggingface import HuggingFaceEndpointEmbeddings

def get_embeddings():
    return HuggingFaceEndpointEmbeddings(
        model="sentence-transformers/all-MiniLM-L6-v2",
        huggingfacehub_api_token=HF_TOKEN
    )
```

**What are Embeddings?**

Think of embeddings as "meaning coordinates":
```
"Fixed Deposit Rate" â†’ [0.23, -0.45, 0.67, ...] (384 numbers)
"FD Interest"        â†’ [0.22, -0.44, 0.68, ...] (very similar!)
"Stock Price"        â†’ [-0.15, 0.89, -0.34, ...] (very different)
```

Texts with similar meanings have similar embedding vectors.

**Why HuggingFace Inference API (not local)?**
```
Local model (sentence-transformers):
âŒ 500MB+ download on first run
âŒ Uses server RAM
âŒ Cold start time

HuggingFace API:
âœ… No download needed
âœ… Runs on HF's infrastructure
âœ… Instant startup
âœ… Free tier available
```

**Why all-MiniLM-L6-v2?**
- Optimized for semantic similarity
- Only 384 dimensions (fast)
- Great quality for its size
- Battle-tested in production

### Step 6: Store in Vector Database (FAISS)

```python
from langchain_community.vectorstores import FAISS

def add_documents(documents: List[Document]):
    embeddings = get_embeddings()
    
    if vector_store is None:
        # First document: Create new store
        vector_store = FAISS.from_documents(documents, embeddings)
    else:
        # Additional documents: Add to existing
        vector_store.add_documents(documents)
    
    # Persist to disk for restart survival
    vector_store.save_local("./vector_store")
```

**What is FAISS?**

FAISS = Facebook AI Similarity Search
- Developed by Meta for finding similar items quickly
- Used at scale (billions of vectors)
- In-memory for speed, but we persist to disk

**How Does Vector Search Work?**

```
User asks: "What is my interest rate?"

1. Convert query to embedding:
   "interest rate" â†’ [0.21, -0.43, 0.65, ...]

2. FAISS finds vectors closest to this:
   Chunk 1: [0.23, -0.45, 0.67, ...] â†’ Distance: 0.02 âœ… Very close!
   Chunk 2: [-0.15, 0.89, -0.34, ...] â†’ Distance: 1.45 âŒ Far away

3. Return top-k closest chunks (k=4 typically)
```

---

# Part 3: The Retrieval Strategy - Hybrid Search

## Why Hybrid Search? (Critical for Interviews!)

**Semantic Search (FAISS) Alone is NOT Enough**

```
document_text = "Policy ID: FP-2024-001"
user_query = "FP-2024-001"

Semantic search thinks:
"FP-2024-001" â‰ˆ random string
"Policy ID" â‰ˆ identification concept

Result: Might NOT find the exact policy ID!
```

**Keyword Search (BM25) Alone is NOT Enough**

```
user_query = "What money can I save on taxes?"
document_text = "Section 80C deduction: â‚¹1.5 lakh"

BM25 looks for exact words:
- "save" not found
- "taxes" not found
- Only "tax" in the document

Result: Might miss relevant sections!
```

**Solution: Hybrid Search = Semantic + Keyword**

```
Semantic (FAISS): Understands meaning
"save taxes" â†’ finds "80C deduction"

Keyword (BM25): Matches exact terms
"FP-2024-001" â†’ finds exact policy ID
```

## BM25 Implementation

```python
from langchain_community.retrievers import BM25Retriever

def update_bm25_corpus(documents: List[Document]):
    bm25_retriever = BM25Retriever.from_documents(
        documents,
        k=4  # Return top 4 matches
    )
```

**What is BM25?**

BM25 = Best Matching 25 (an improved TF-IDF)

```
Scoring formula (simplified):
score = term_frequency Ã— inverse_document_frequency

high score = term appears often in this doc, but rarely in other docs
```

## Reciprocal Rank Fusion (RRF)

**Problem:** FAISS and BM25 return different scores. How to combine?

```
FAISS results:  [Doc A (0.95), Doc B (0.87), Doc C (0.82)]
BM25 results:   [Doc B (25.3), Doc D (18.7), Doc A (12.1)]

Can't just add scores - different scales!
```

**Solution: RRF - Combine by RANK, not score**

```python
from langchain_classic.retrievers import EnsembleRetriever

def get_hybrid_retriever():
    faiss_retriever = get_faiss_retriever(k=4)
    bm25_retriever = get_bm25_retriever(k=4)
    
    return EnsembleRetriever(
        retrievers=[faiss_retriever, bm25_retriever],
        weights=[0.5, 0.5]  # Equal importance
    )
```

**How RRF Works:**

```
Document A: FAISS rank 1, BM25 rank 3
           RRF score = 1/(60+1) + 1/(60+3) = 0.016 + 0.016 = 0.032

Document B: FAISS rank 2, BM25 rank 1
           RRF score = 1/(60+2) + 1/(60+1) = 0.016 + 0.016 = 0.032

Document D: FAISS rank âˆ, BM25 rank 2
           RRF score = 0 + 1/(60+2) = 0 + 0.016 = 0.016

Final ranking: A and B tied at top, D lower
```

**Why 60 in the formula?**
- It's a smoothing constant
- Prevents low-ranked documents from having zero contribution
- Standard value from the RRF paper

---

# Part 4: The RAG Agent - Decision Making

## Why Use an Agent?

**Without Agent (Simple RAG):**
```
User: "What's the Nifty 50 today?"
System: [Searches uploaded documents]
        [Finds nothing relevant]
Answer: "I couldn't find that information."

FAIL! The user wanted live web data, not documents.
```

**With Agent:**
```
User: "What's the Nifty 50 today?"
Agent: "This asks for live/current data â†’ use web_search"
       [Searches Tavily for Nifty 50]
Answer: "Nifty 50 is at 26,129.60 INR"

SUCCESS! Agent picked the right tool.
```

## Tool Selection Logic

```python
# The agent has access to two tools:

TOOLS:
1. document_search: Search user's uploaded PDFs/policies
   Use for: "my policy", "my documents", personal files

2. web_search: Get real-time web data
   Use for: "today", "current", "latest", market data
```

## OpenAI Tool Calling (JSON-based, No Parsing Errors)

Old approach (ReAct - text parsing):
```
Thought: I should search documents
Action: document_search
Action Input: "interest rate"
              â†‘ Text parsing is fragile!
```

New approach (Tool Calling):
```python
def get_llm_with_tools():
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    tools = [document_search_tool, web_search_tool]
    
    # Bind tools for structured JSON calling
    return llm.bind_tools(tools)
```

OpenAI returns structured JSON:
```json
{
  "tool_calls": [{
    "name": "document_search",
    "args": {"query": "interest rate"}
  }]
}
```

**Why is this better?**
- No text parsing â†’ no parsing errors
- Model can't "format" the call wrong
- JSON is validated automatically
- Much more reliable in production

## The Agent Loop

```python
async def run_agent(query: str, session_id: str):
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        # ... chat history ...
        {"role": "user", "content": query}
    ]
    
    for _ in range(max_iterations):
        response = await llm_with_tools.ainvoke(messages)
        
        if not response.tool_calls:
            # No tools needed â†’ return final answer
            return response.content
        
        # Execute each tool call
        for tool_call in response.tool_calls:
            tool = tool_map[tool_call["name"]]
            result = tool.invoke(tool_call["args"])
            
            # Add result to messages
            messages.append(ToolMessage(
                content=str(result),
                tool_call_id=tool_call["id"]
            ))
        
        # Loop continues: LLM sees tool results, decides next action
```

---

# Part 5: Conversation Memory

## Why Memory Matters

```
Without Memory:
User: "What's my FD rate?"
AI: "7.5% per annum"

User: "Is that good?"
AI: "Is WHAT good? I need more context."
    â†‘ Lost the conversation!

With Memory:
User: "What's my FD rate?"
AI: "7.5% per annum"

User: "Is that good?"
AI: "7.5% is above the current market average of 6.5%, so yes!"
    â†‘ Remembers the context!
```

## Implementation: Session-Based Memory

```python
from langchain_classic.memory import ConversationBufferWindowMemory

# Store sessions globally (in production: use Redis)
sessions: Dict[str, ConversationBufferWindowMemory] = {}

def get_or_create_memory(session_id: str):
    if session_id not in sessions:
        sessions[session_id] = ConversationBufferWindowMemory(
            k=5,  # Remember last 5 exchanges
            memory_key="chat_history",
            return_messages=True
        )
    return sessions[session_id]
```

**Why Window Memory (not full)?**
- Context windows have limits
- Older messages are less relevant
- Prevents token explosion
- k=5 = 10 messages (5 user + 5 assistant)

## Using Memory in the Agent

```python
async def run_agent(query, session_id):
    memory = get_or_create_memory(session_id)
    
    # Load past conversation
    chat_history = memory.load_memory_variables({}).get("chat_history", [])
    
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    
    # Add history to messages
    for msg in chat_history[-10:]:
        role = "user" if msg.type == "human" else "assistant"
        messages.append({"role": role, "content": msg.content})
    
    # Add current query
    messages.append({"role": "user", "content": query})
    
    # ... run agent ...
    
    # Save to memory after response
    memory.save_context(
        {"input": query},
        {"output": final_answer}
    )
```

---

# Part 6: The API Layer (FastAPI)

## Endpoint Overview

```python
# Health check (for load balancers, monitoring)
@app.get("/health")
@app.get("/api/health")
async def health_check():
    return {"status": "healthy", "service": "FinSync Pro"}

# Upload PDF
@app.post("/api/upload")
async def upload_document(file: UploadFile):
    # Process and index the document

# Send chat message
@app.post("/api/chat")
async def chat(request: ChatRequest):
    # Run RAG agent and return response

# List indexed documents
@app.get("/api/documents")
async def get_documents():
    # Return list of all indexed docs
```

## CORS Configuration

```python
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production: specific domains
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

**Why CORS?**
- Frontend (localhost:5173) and backend (localhost:8000) are different origins
- Browsers block cross-origin requests by default
- CORS headers tell browser "this is allowed"

---

# Part 7: Configuration & Environment

```python
# config.py - Centralized configuration

# API Keys (from environment)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
HF_TOKEN = os.getenv("HF_TOKEN")

# LLM Settings
LLM_MODEL = "gpt-4o-mini"  # Fast and cheap
LLM_TEMPERATURE = 0        # Deterministic output

# Retriever Weights
FAISS_WEIGHT = 0.5   # Semantic search
BM25_WEIGHT = 0.5    # Keyword search

# Chunking
CHUNK_SIZE = 500
CHUNK_OVERLAP = 50

# Embedding
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
```

**Why Environment Variables for API Keys?**
- Never hardcode secrets in code
- Different keys for dev/staging/production
- .env files are gitignored

---

# Part 8: Interview Questions & Answers

## Question 1: "Why did you choose FAISS over Pinecone/Weaviate?"

**Answer:**
"FAISS was chosen for this project because:
1. It's in-memory, which is extremely fast for smaller datasets
2. No external service dependency - runs locally
3. Free and open-source (Meta developed it)
4. Perfect for demonstration and POC
5. Can be replaced with Pinecone in production for scale"

## Question 2: "What happens when embeddings don't find relevant results?"

**Answer:**
"The hybrid search strategy handles this:
1. Semantic search (FAISS) might miss exact terms like policy IDs
2. BM25 keyword search catches those exact matches
3. RRF fusion combines both, so we rarely get complete misses
4. If both fail, the agent can fall back to web search if appropriate
5. If nothing is found, we return a clear 'not found' message rather than hallucinating"

## Question 3: "How do you handle multi-turn conversations?"

**Answer:**
"We implemented ConversationBufferWindowMemory with k=5 exchanges:
1. Each session has a unique ID stored in the frontend
2. Memory is stored server-side (in production: Redis)
3. Before each LLM call, we load the last 10 messages
4. After each response, we save the exchange
5. The window limit prevents context overflow"

## Question 4: "Why gpt-4o-mini instead of gpt-4o?"

**Answer:**
"For the agent's task of tool selection and answer generation:
1. gpt-4o-mini is 10x cheaper and 3x faster
2. The task isn't complex - it's selecting the right tool and formatting answers
3. The 'intelligence' comes from retrieval, not the LLM itself
4. Temperature=0 ensures consistent behavior
5. For complex reasoning tasks, we could swap to gpt-4o"

## Question 5: "What's the biggest challenge in RAG systems?"

**Answer:**
"The biggest challenges are:
1. **Chunking strategy** - too small loses context, too large wastes tokens
2. **Semantic gap** - user says 'save money' but doc says 'tax deduction'
3. **Citation accuracy** - ensuring the answer actually comes from cited sources
4. **Latency** - multiple API calls (embedding + LLM) add up
5. **Memory limits** - balancing context window with conversation length"

## Question 6: "How would you scale this to millions of documents?"

**Answer:**
"Several changes would be needed:
1. Replace FAISS with Pinecone/Weaviate (vector database as a service)
2. Add Redis for session memory (currently in-memory)
3. Queue document processing with Celery/RabbitMQ
4. Add document chunking workers (horizontal scaling)
5. Implement caching for common queries
6. Use async embedding batching"

## Question 7: "What if the LLM hallucinates despite having context?"

**Answer:**
"We minimize hallucination risk through:
1. Temperature=0 for deterministic outputs
2. System prompt that says 'only answer based on retrieved content'
3. Structured output that must cite its sources
4. Post-processing validation (check if answer appears in context)
5. User-visible citations so they can verify"

---

# Part 9: The Complete Request Flow (Diagram)

```
User types: "What's my FD rate?"
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frontend (React)            â”‚
â”‚ POST /api/chat              â”‚
â”‚ {query: "...", session_id}  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FastAPI /api/chat           â”‚
â”‚ Receives request            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Load Session Memory         â”‚
â”‚ Get last 5 exchanges        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Build Messages Array        â”‚
â”‚ [system, history, query]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Call GPT-4o-mini            â”‚
â”‚ with bound tools            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GPT returns tool_call:      â”‚
â”‚ {name: document_search,     â”‚
â”‚  args: {query: "FD rate"}}  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Execute Hybrid Search       â”‚
â”‚ FAISS + BM25 â†’ RRF          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Return top 4 chunks         â”‚
â”‚ "FD Rate: 7.5%..."          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Add ToolMessage to chat     â”‚
â”‚ Call GPT again              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GPT generates final answer: â”‚
â”‚ "Your FD rate is 7.5%"      â”‚
â”‚ (No more tool calls)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Save to memory              â”‚
â”‚ Return to frontend          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frontend displays:          â”‚
â”‚ Answer + Sources + Citation â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Part 10: LangChain Package Breakdown

| Package | What We Use From It | Purpose |
|---------|---------------------|---------|
| `langchain-huggingface` | `HuggingFaceEndpointEmbeddings` | Connect to HF Inference API for embeddings |
| `langchain-community` | `FAISS`, `BM25Retriever`, `TavilySearchResults` | Vector store, keyword search, web search |
| `langchain-text-splitters` | `RecursiveCharacterTextSplitter` | Break documents into chunks |
| `langchain-core` | `Document`, `Tool`, `PromptTemplate` | Base interfaces and types |
| `langchain-openai` | `ChatOpenAI` | Connect to OpenAI GPT models |
| `langchain-classic` | `AgentExecutor`, `EnsembleRetriever`, `Memory` | Agent orchestration, retriever fusion, conversation memory |

---

# Final Checklist Before the Interview

- [ ] Explain what RAG is and why it's needed for financial AI
- [ ] Walk through the document ingestion pipeline (upload â†’ chunk â†’ embed â†’ store)
- [ ] Explain why we use hybrid search (FAISS + BM25)
- [ ] Describe how RRF combines results from different retrievers
- [ ] Explain how the agent decides which tool to use
- [ ] Describe how conversation memory maintains context
- [ ] Know why we chose specific technologies (FAISS, gpt-4o-mini, HF API)
- [ ] Be ready to discuss scaling strategies
- [ ] Understand the complete request flow from user input to response

---

**Good luck with the interview! ğŸš€**

Remember: The key is not just knowing WHAT was built, but WHY each decision was made. Interviewers love hearing the reasoning behind technical choices.
